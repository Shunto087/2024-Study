{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This script was written at Yoshimi Lab in Tohoku University, Material Science and Engineering Department"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Script Discription\n",
    "\n",
    "This script is for generating synthesis images from stored DCGAN models."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Import necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yoshi\\anaconda3\\envs\\stylegan_python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from math import log2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Check the availability of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 23 17:38:02 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 522.06       Driver Version: 522.06       CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000   WDDM  | 00000000:01:00.0  On |                  Off |\n",
      "| 67%   84C    P2   266W / 300W |  26063MiB / 49140MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       556    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      2828    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      4156    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5620    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6576    C+G   ...\\ApplicationFrameHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7588    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10116    C+G   ...oft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     13392    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     14136    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15048      C   ...legan_python39\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     15556    C+G   ...urrent\\LogiOptionsMgr.exe    N/A      |\n",
      "|    0   N/A  N/A     15580    C+G   ...e\\Current\\LogiOverlay.exe    N/A      |\n",
      "|    0   N/A  N/A     15684    C+G   ...03.112\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     16064    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     20088    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21468    C+G   ...03.112\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     22340    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     26764    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     27104    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     32308    C+G   ...ws\\System32\\ShellHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "IMG_CHANNELS = 1\n",
    "STEP = 6\n",
    "FIRST_EPOCH = 10\n",
    "LAST_EPOCH = 160\n",
    "EPOCH_INTERVAL = 10\n",
    "NUM_IMAGES = 2182\n",
    "\n",
    "# Optional\n",
    "Z_DIM = 512\n",
    "W_DIM = 512\n",
    "IN_CHANNELS = 512\n",
    "factors = [1,1,1,1/2,1/4,1/8,1/16,1/32]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True):\n",
    "        super(EqualizedLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "        nn.init.normal_(self.linear.weight, mean=0.0, std=1.0 / np.sqrt(in_dim))\n",
    "        if bias:\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "        self.scale = (2 / in_dim) ** 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim = 1, keepdim = True) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualizedLinear(w_dim, w_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualizedLinear(w_dim, w_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualizedLinear(w_dim, w_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualizedLinear(w_dim, w_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualizedLinear(w_dim, w_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualizedLinear(w_dim, w_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            EqualizedLinear(w_dim, w_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    \n",
    "    def forward(self,z):\n",
    "        return self.mapping(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.style_scale   = nn.Linear(w_dim, channels)\n",
    "        self.style_bias    = nn.Linear(w_dim, channels)\n",
    "\n",
    "    def forward(self,x,w):\n",
    "        x = self.instance_norm(x)\n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)\n",
    "        style_bias  = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style_scale * x + style_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedConv2d(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1,bias = True\n",
    "    ):\n",
    "        super(EqualizedConv2d, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        fan_in = in_channels * kernel_size * kernel_size\n",
    "        self.scale = 1 / math.sqrt(fan_in)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.conv2d(\n",
    "            x, self.weight * self.scale, bias=self.bias, stride=self.stride, padding=self.padding\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(NoiseInjection, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1,channels,1,1))\n",
    "\n",
    "    def forward(self, x, noise=None):\n",
    "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device = x.device)\n",
    "        return x + self.weight * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, w_dim):\n",
    "        super(GeneratorBlock, self).__init__()\n",
    "        self.conv1 = EqualizedConv2d(in_channels, out_channels)\n",
    "        self.conv2 = EqualizedConv2d(out_channels, out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.noise_injection1 = NoiseInjection(out_channels)\n",
    "        self.noise_injection2 = NoiseInjection(out_channels)\n",
    "        self.adain1 = AdaIN(out_channels, w_dim)\n",
    "        self.adain2 = AdaIN(out_channels, w_dim)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = self.adain1(self.activation(self.noise_injection1(self.conv1(x))), w)\n",
    "        x = self.adain2(self.activation(self.noise_injection2(self.conv2(x))), w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim, in_channels, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.constant = nn.Parameter(torch.ones(1, in_channels, 4, 4))\n",
    "        self.mapping_network = MappingNetwork(z_dim, w_dim)\n",
    "        self.first_adain1 = AdaIN(in_channels, w_dim)\n",
    "        self.first_adain2 = AdaIN(in_channels, w_dim)\n",
    "        self.first_noise1 = NoiseInjection(in_channels)\n",
    "        self.first_noise2 = NoiseInjection(in_channels)\n",
    "        self.first_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.first_torgb = EqualizedConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.prog_blocks, self.to_rgbs = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.first_torgb])\n",
    "        )\n",
    "        \n",
    "        for i in range(len(factors)-1):\n",
    "            in_channels_c = int(in_channels * factors[i])\n",
    "            out_channels_c = int(in_channels * factors[i+1])\n",
    "            self.prog_blocks.append(GeneratorBlock(in_channels_c, out_channels_c, w_dim))\n",
    "            self.to_rgbs.append(EqualizedConv2d(out_channels_c, img_channels, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, z, alpha, steps):\n",
    "        w = self.mapping_network(z)\n",
    "        x = self.first_adain1(self.first_noise1(self.constant), w)\n",
    "        x = self.first_conv(x)\n",
    "        out = self.first_adain2(self.activation(self.first_noise2(x)), w)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.first_torgb(x)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode='bilinear')\n",
    "            out = self.prog_blocks[step](upscaled, w)\n",
    "\n",
    "        final_upscaled = self.to_rgbs[steps-1](upscaled)\n",
    "        final_out = self.to_rgbs[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n",
      "Saved 2182 images to 'synthesis_images/'\n"
     ]
    }
   ],
   "source": [
    "def generate_from_checkpoint(checkpoint_path, step, epoch, num_images=10):\n",
    "    generator = Generator(Z_DIM, W_DIM, IN_CHANNELS, IMG_CHANNELS).to(DEVICE)\n",
    "    generator.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "    generator.eval()\n",
    "\n",
    "    alpha = 1.0\n",
    "    os.makedirs(f\"synthesis_images/step{step}/epoch{epoch}\", exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_images):\n",
    "            noise = torch.randn(1, Z_DIM).to(DEVICE)\n",
    "            img = generator(noise, alpha, step)\n",
    "            save_image(img * 0.5 + 0.5, f\"synthesis_images/step{step}/epoch{epoch}/synthesis_image_{i}.tif\")\n",
    "    print(f\"Saved {num_images} images to 'synthesis_images/'\")\n",
    "\n",
    "\n",
    "def main(epoch):\n",
    "    step = STEP\n",
    "    epoch = epoch\n",
    "    checkpoint_path = f\"./saved_models/step{step}/generator_step{step}_epoch{epoch}.pth\"\n",
    "    num_images = NUM_IMAGES\n",
    "    generate_from_checkpoint(checkpoint_path, step, epoch, num_images)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(FIRST_EPOCH, LAST_EPOCH+10, EPOCH_INTERVAL):\n",
    "        main(epoch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This script was written at Yoshimi Lab in Tohoku University, Material Science and Engineering Department"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan_python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
